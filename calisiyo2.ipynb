{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = re.compile(\"\\s+\")\n",
    "base_path = './data/img/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_bboxes(list_all, split='train'):\n",
    "    lst = [(line[0], line[1], line[3], line[2]) for line in list_all if line[2] == split]\n",
    "    lst = [(\"\".join(line[0].split('/')[0] + '/' + line[3] + '/' + line[1] + line[0][3:]), line[1], line[2]) for line in lst]\n",
    "    lst_shape = [cv2.imread('./data/' + line[0]).shape for line in lst]\n",
    "    lst = [(line[0], line[1], (round(line[2][0] / shape[1], 2), round(line[2][1] / shape[0], 2), round(line[2][2] / shape[1], 2), round(line[2][3] / shape[0], 2), shape)) for line, shape in zip(lst, lst_shape)]\n",
    "    dict_ = {\"/\".join(line[0].split('/')[2:]): {'x1': line[2][0], 'y1': line[2][1], 'x2': line[2][2], 'y2': line[2][3], 'shape': line[2][4]} for line in lst}\n",
    "    return dict_\n",
    "def get_dict_bboxes():\n",
    "    with open('./data/anno/list_category_img.txt', 'r') as category_img_file, \\\n",
    "            open('./data/anno/list_eval_partition.txt', 'r') as eval_partition_file, \\\n",
    "            open('./data/anno/list_bbox.txt', 'r') as bbox_file:\n",
    "        list_category_img = [line.rstrip('\\n') for line in category_img_file][2:]\n",
    "        list_eval_partition = [line.rstrip('\\n') for line in eval_partition_file][2:]\n",
    "        list_bbox = [line.rstrip('\\n') for line in bbox_file][2:]\n",
    "\n",
    "        list_category_img = [splitter.split(line) for line in list_category_img]\n",
    "        list_eval_partition = [splitter.split(line) for line in list_eval_partition]\n",
    "        list_bbox = [splitter.split(line) for line in list_bbox]\n",
    "\n",
    "        list_all = [(k[0], k[0].split('/')[1].split('_')[-1], v[1], (int(b[1]), int(b[2]), int(b[3]), int(b[4])))\n",
    "                    for k, v, b in zip(list_category_img, list_eval_partition, list_bbox)]\n",
    "\n",
    "        list_all.sort(key=lambda x: x[1])\n",
    "\n",
    "        dict_train = create_dict_bboxes(list_all)\n",
    "        dict_val = create_dict_bboxes(list_all, split='val')\n",
    "        dict_test = create_dict_bboxes(list_all, split='test')\n",
    "\n",
    "        return dict_train, dict_val, dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import DirectoryIterator, ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model_resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_resnet.layers[:-12]:\n",
    "    # 6 - 12 - 18 have been tried. 12 is the best.\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model_resnet.output\n",
    "x = Dense(512, activation='elu', kernel_regularizer=l2(0.001))(x)\n",
    "y = Dense(46, activation='softmax', name='img')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bbox = model_resnet.output\n",
    "x_bbox = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x_bbox)\n",
    "x_bbox = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x_bbox)\n",
    "bbox = Dense(4, kernel_initializer='normal', name='bbox')(x_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Model(inputs=model_resnet.input,outputs=[y, bbox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr=0.0001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicates `model` on 8 GPUs.\n",
    "# This assumes that your machine has 8 available GPUs.\n",
    "final_model = multi_gpu_model(final_model, gpus=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.compile(optimizer=opt,\n",
    "                    loss={'img': 'categorical_crossentropy',\n",
    "                          'bbox': 'mean_squared_error'},\n",
    "                    metrics={'img': ['accuracy', 'top_k_categorical_accuracy'], # default: top-5\n",
    "                             'bbox': ['mse']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rotation_range=30.,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectoryIteratorWithBoundingBoxes(DirectoryIterator):\n",
    "    def __init__(self, directory, image_data_generator, bounding_boxes: dict = None, target_size=(256, 256),\n",
    "                 color_mode: str = 'rgb', classes=None, class_mode: str = 'categorical', batch_size: int = 32,\n",
    "                 shuffle: bool = True, seed=None, data_format=None, save_to_dir=None,\n",
    "                 save_prefix: str = '', save_format: str = 'jpeg', follow_links: bool = False):\n",
    "        super().__init__(directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size,\n",
    "                         shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links)\n",
    "        self.bounding_boxes = bounding_boxes\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        # Returns\n",
    "            The next batch.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())\n",
    "        locations = np.zeros((len(batch_x),) + (4,), dtype=K.floatx())\n",
    "\n",
    "        grayscale = self.color_mode == 'grayscale'\n",
    "        # build batch of image data\n",
    "        for i, j in enumerate(index_array):\n",
    "            fname = self.filenames[j]\n",
    "            img = image.load_img(os.path.join(self.directory, fname),\n",
    "                                 grayscale=grayscale,\n",
    "                                 target_size=self.target_size)\n",
    "            x = image.img_to_array(img, data_format=self.data_format)\n",
    "            x = self.image_data_generator.random_transform(x)\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "            batch_x[i] = x\n",
    "\n",
    "            if self.bounding_boxes is not None:\n",
    "                bounding_box = self.bounding_boxes[fname]\n",
    "                locations[i] = np.asarray(\n",
    "                    [bounding_box['x1'], bounding_box['y1'], bounding_box['x2'], bounding_box['y2']],\n",
    "                    dtype=K.floatx())\n",
    "        # optionally save augmented images to disk for debugging purposes\n",
    "        # build batch of labels\n",
    "        if self.class_mode == 'sparse':\n",
    "            batch_y = self.classes[index_array]\n",
    "        elif self.class_mode == 'binary':\n",
    "            batch_y = self.classes[index_array].astype(K.floatx())\n",
    "        elif self.class_mode == 'categorical':\n",
    "            batch_y = np.zeros((len(batch_x), 46), dtype=K.floatx())\n",
    "            for i, label in enumerate(self.classes[index_array]):\n",
    "                batch_y[i, label] = 1.\n",
    "        else:\n",
    "            return batch_x\n",
    "\n",
    "        if self.bounding_boxes is not None:\n",
    "            return batch_x, [batch_y, locations]\n",
    "        else:\n",
    "            return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train, dict_val, dict_test = get_dict_bboxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 209222 images belonging to 46 classes.\n",
      "Found 40000 images belonging to 46 classes.\n"
     ]
    }
   ],
   "source": [
    "train_iterator = DirectoryIteratorWithBoundingBoxes(\"./data/img/train\", train_datagen, bounding_boxes=dict_train, target_size=(200, 200))\n",
    "test_iterator = DirectoryIteratorWithBoundingBoxes(\"./data/img/val\", test_datagen, bounding_boxes=dict_val,target_size=(200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
    "                               patience=12,\n",
    "                               factor=0.5,\n",
    "                               verbose=1)\n",
    "tensorboard = TensorBoard(log_dir='./logs')\n",
    "early_stopper = EarlyStopping(monitor='val_loss',\n",
    "                              patience=30,\n",
    "                              verbose=1)\n",
    "checkpoint = ModelCheckpoint('./models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generator(iterator):\n",
    "    while True:\n",
    "        batch_x, batch_y = iterator.next()\n",
    "        yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 86s - loss: 4.1876 - img_loss: 2.3149 - bbox_loss: 0.0332 - img_acc: 0.3397 - img_top_k_categorical_accuracy: 0.7174 - bbox_mean_squared_error: 0.0332 - val_loss: 5.0035 - val_img_loss: 3.0033 - val_bbox_loss: 0.1615 - val_img_acc: 0.2000 - val_img_top_k_categorical_accuracy: 0.5487 - val_bbox_mean_squared_error: 0.1615\n",
      "Epoch 2/200\n",
      " - 83s - loss: 4.4182 - img_loss: 2.5466 - bbox_loss: 0.0338 - img_acc: 0.3009 - img_top_k_categorical_accuracy: 0.6166 - bbox_mean_squared_error: 0.0338 - val_loss: 4.8454 - val_img_loss: 2.8427 - val_bbox_loss: 0.1658 - val_img_acc: 0.2812 - val_img_top_k_categorical_accuracy: 0.5906 - val_bbox_mean_squared_error: 0.1658\n",
      "Epoch 3/200\n",
      " - 83s - loss: 4.4000 - img_loss: 2.5329 - bbox_loss: 0.0311 - img_acc: 0.3058 - img_top_k_categorical_accuracy: 0.6215 - bbox_mean_squared_error: 0.0311 - val_loss: 4.6408 - val_img_loss: 2.6454 - val_bbox_loss: 0.1604 - val_img_acc: 0.2631 - val_img_top_k_categorical_accuracy: 0.6369 - val_bbox_mean_squared_error: 0.1604\n",
      "Epoch 4/200\n",
      " - 83s - loss: 4.2863 - img_loss: 2.4207 - bbox_loss: 0.0314 - img_acc: 0.3431 - img_top_k_categorical_accuracy: 0.6806 - bbox_mean_squared_error: 0.0314 - val_loss: 4.7218 - val_img_loss: 2.7019 - val_bbox_loss: 0.1866 - val_img_acc: 0.2731 - val_img_top_k_categorical_accuracy: 0.6144 - val_bbox_mean_squared_error: 0.1866\n",
      "Epoch 5/200\n",
      " - 83s - loss: 4.2222 - img_loss: 2.3613 - bbox_loss: 0.0285 - img_acc: 0.3359 - img_top_k_categorical_accuracy: 0.6936 - bbox_mean_squared_error: 0.0285 - val_loss: 4.6528 - val_img_loss: 2.6605 - val_bbox_loss: 0.1608 - val_img_acc: 0.3069 - val_img_top_k_categorical_accuracy: 0.6519 - val_bbox_mean_squared_error: 0.1608\n",
      "Epoch 6/200\n",
      " - 83s - loss: 4.1333 - img_loss: 2.2751 - bbox_loss: 0.0276 - img_acc: 0.3516 - img_top_k_categorical_accuracy: 0.7270 - bbox_mean_squared_error: 0.0276 - val_loss: 4.4144 - val_img_loss: 2.4078 - val_bbox_loss: 0.1769 - val_img_acc: 0.3419 - val_img_top_k_categorical_accuracy: 0.6925 - val_bbox_mean_squared_error: 0.1769\n",
      "Epoch 7/200\n",
      " - 83s - loss: 4.1461 - img_loss: 2.2898 - bbox_loss: 0.0274 - img_acc: 0.3774 - img_top_k_categorical_accuracy: 0.6890 - bbox_mean_squared_error: 0.0274 - val_loss: 4.5786 - val_img_loss: 2.5623 - val_bbox_loss: 0.1882 - val_img_acc: 0.2769 - val_img_top_k_categorical_accuracy: 0.6456 - val_bbox_mean_squared_error: 0.1882\n",
      "Epoch 8/200\n",
      " - 83s - loss: 4.0913 - img_loss: 2.2382 - bbox_loss: 0.0259 - img_acc: 0.3806 - img_top_k_categorical_accuracy: 0.7226 - bbox_mean_squared_error: 0.0259 - val_loss: 4.4091 - val_img_loss: 2.4180 - val_bbox_loss: 0.1649 - val_img_acc: 0.3300 - val_img_top_k_categorical_accuracy: 0.7163 - val_bbox_mean_squared_error: 0.1649\n",
      "Epoch 9/200\n",
      " - 83s - loss: 4.1252 - img_loss: 2.2736 - bbox_loss: 0.0263 - img_acc: 0.3650 - img_top_k_categorical_accuracy: 0.7147 - bbox_mean_squared_error: 0.0263 - val_loss: 4.4186 - val_img_loss: 2.4275 - val_bbox_loss: 0.1666 - val_img_acc: 0.3806 - val_img_top_k_categorical_accuracy: 0.7094 - val_bbox_mean_squared_error: 0.1666\n",
      "Epoch 10/200\n",
      " - 83s - loss: 4.1240 - img_loss: 2.2754 - bbox_loss: 0.0251 - img_acc: 0.3601 - img_top_k_categorical_accuracy: 0.7127 - bbox_mean_squared_error: 0.0251 - val_loss: 4.4543 - val_img_loss: 2.4538 - val_bbox_loss: 0.1778 - val_img_acc: 0.3600 - val_img_top_k_categorical_accuracy: 0.7013 - val_bbox_mean_squared_error: 0.1778\n",
      "Epoch 11/200\n",
      " - 83s - loss: 4.0628 - img_loss: 2.2157 - bbox_loss: 0.0254 - img_acc: 0.3817 - img_top_k_categorical_accuracy: 0.7344 - bbox_mean_squared_error: 0.0254 - val_loss: 4.3824 - val_img_loss: 2.3862 - val_bbox_loss: 0.1753 - val_img_acc: 0.3438 - val_img_top_k_categorical_accuracy: 0.7219 - val_bbox_mean_squared_error: 0.1753\n",
      "Epoch 12/200\n",
      " - 83s - loss: 4.0822 - img_loss: 2.2384 - bbox_loss: 0.0238 - img_acc: 0.3655 - img_top_k_categorical_accuracy: 0.7300 - bbox_mean_squared_error: 0.0238 - val_loss: 4.3667 - val_img_loss: 2.3731 - val_bbox_loss: 0.1744 - val_img_acc: 0.3656 - val_img_top_k_categorical_accuracy: 0.7356 - val_bbox_mean_squared_error: 0.1744\n",
      "Epoch 13/200\n",
      " - 83s - loss: 4.0156 - img_loss: 2.1725 - bbox_loss: 0.0248 - img_acc: 0.3923 - img_top_k_categorical_accuracy: 0.7508 - bbox_mean_squared_error: 0.0248 - val_loss: 4.3918 - val_img_loss: 2.3951 - val_bbox_loss: 0.1793 - val_img_acc: 0.3600 - val_img_top_k_categorical_accuracy: 0.7281 - val_bbox_mean_squared_error: 0.1793\n",
      "Epoch 14/200\n",
      " - 83s - loss: 4.0478 - img_loss: 2.2076 - bbox_loss: 0.0237 - img_acc: 0.4054 - img_top_k_categorical_accuracy: 0.7086 - bbox_mean_squared_error: 0.0237 - val_loss: 4.4404 - val_img_loss: 2.4379 - val_bbox_loss: 0.1868 - val_img_acc: 0.3656 - val_img_top_k_categorical_accuracy: 0.7406 - val_bbox_mean_squared_error: 0.1868\n",
      "Epoch 15/200\n",
      " - 83s - loss: 3.9921 - img_loss: 2.1527 - bbox_loss: 0.0246 - img_acc: 0.4019 - img_top_k_categorical_accuracy: 0.7498 - bbox_mean_squared_error: 0.0246 - val_loss: 4.4566 - val_img_loss: 2.4604 - val_bbox_loss: 0.1823 - val_img_acc: 0.3594 - val_img_top_k_categorical_accuracy: 0.7269 - val_bbox_mean_squared_error: 0.1823\n",
      "Epoch 16/200\n",
      " - 83s - loss: 3.9666 - img_loss: 2.1293 - bbox_loss: 0.0243 - img_acc: 0.3779 - img_top_k_categorical_accuracy: 0.7533 - bbox_mean_squared_error: 0.0243 - val_loss: 4.5850 - val_img_loss: 2.5901 - val_bbox_loss: 0.1828 - val_img_acc: 0.3475 - val_img_top_k_categorical_accuracy: 0.6919 - val_bbox_mean_squared_error: 0.1828\n",
      "Epoch 17/200\n",
      " - 83s - loss: 3.8860 - img_loss: 2.0518 - bbox_loss: 0.0229 - img_acc: 0.4261 - img_top_k_categorical_accuracy: 0.7606 - bbox_mean_squared_error: 0.0229 - val_loss: 4.3959 - val_img_loss: 2.3873 - val_bbox_loss: 0.1982 - val_img_acc: 0.3987 - val_img_top_k_categorical_accuracy: 0.7469 - val_bbox_mean_squared_error: 0.1982\n",
      "Epoch 18/200\n",
      " - 83s - loss: 3.9567 - img_loss: 2.1248 - bbox_loss: 0.0223 - img_acc: 0.3909 - img_top_k_categorical_accuracy: 0.7620 - bbox_mean_squared_error: 0.0223 - val_loss: 4.3613 - val_img_loss: 2.3689 - val_bbox_loss: 0.1838 - val_img_acc: 0.3856 - val_img_top_k_categorical_accuracy: 0.7631 - val_bbox_mean_squared_error: 0.1838\n",
      "Epoch 19/200\n",
      " - 83s - loss: 3.9188 - img_loss: 2.0887 - bbox_loss: 0.0223 - img_acc: 0.4250 - img_top_k_categorical_accuracy: 0.7476 - bbox_mean_squared_error: 0.0223 - val_loss: 4.4970 - val_img_loss: 2.4942 - val_bbox_loss: 0.1959 - val_img_acc: 0.3656 - val_img_top_k_categorical_accuracy: 0.7481 - val_bbox_mean_squared_error: 0.1959\n",
      "Epoch 20/200\n",
      " - 83s - loss: 3.9104 - img_loss: 2.0809 - bbox_loss: 0.0235 - img_acc: 0.4146 - img_top_k_categorical_accuracy: 0.7784 - bbox_mean_squared_error: 0.0235 - val_loss: 4.3105 - val_img_loss: 2.3444 - val_bbox_loss: 0.1609 - val_img_acc: 0.3775 - val_img_top_k_categorical_accuracy: 0.7350 - val_bbox_mean_squared_error: 0.1609\n",
      "Epoch 21/200\n",
      " - 83s - loss: 3.8154 - img_loss: 1.9888 - bbox_loss: 0.0224 - img_acc: 0.4458 - img_top_k_categorical_accuracy: 0.7781 - bbox_mean_squared_error: 0.0224 - val_loss: 4.4009 - val_img_loss: 2.4224 - val_bbox_loss: 0.1751 - val_img_acc: 0.4006 - val_img_top_k_categorical_accuracy: 0.7794 - val_bbox_mean_squared_error: 0.1751\n",
      "Epoch 22/200\n",
      " - 83s - loss: 3.9066 - img_loss: 2.0821 - bbox_loss: 0.0220 - img_acc: 0.3988 - img_top_k_categorical_accuracy: 0.7719 - bbox_mean_squared_error: 0.0220 - val_loss: 4.5150 - val_img_loss: 2.5330 - val_bbox_loss: 0.1803 - val_img_acc: 0.3469 - val_img_top_k_categorical_accuracy: 0.7556 - val_bbox_mean_squared_error: 0.1803\n",
      "Epoch 23/200\n",
      " - 83s - loss: 3.8776 - img_loss: 2.0554 - bbox_loss: 0.0214 - img_acc: 0.3996 - img_top_k_categorical_accuracy: 0.7615 - bbox_mean_squared_error: 0.0214 - val_loss: 4.1665 - val_img_loss: 2.2074 - val_bbox_loss: 0.1592 - val_img_acc: 0.4100 - val_img_top_k_categorical_accuracy: 0.7488 - val_bbox_mean_squared_error: 0.1592\n",
      "Epoch 24/200\n",
      " - 83s - loss: 3.8098 - img_loss: 1.9896 - bbox_loss: 0.0211 - img_acc: 0.4188 - img_top_k_categorical_accuracy: 0.7804 - bbox_mean_squared_error: 0.0211 - val_loss: 4.2784 - val_img_loss: 2.2928 - val_bbox_loss: 0.1873 - val_img_acc: 0.4000 - val_img_top_k_categorical_accuracy: 0.7887 - val_bbox_mean_squared_error: 0.1873\n",
      "Epoch 25/200\n",
      " - 83s - loss: 3.7687 - img_loss: 1.9511 - bbox_loss: 0.0203 - img_acc: 0.4440 - img_top_k_categorical_accuracy: 0.7780 - bbox_mean_squared_error: 0.0203 - val_loss: 4.2976 - val_img_loss: 2.3455 - val_bbox_loss: 0.1556 - val_img_acc: 0.4338 - val_img_top_k_categorical_accuracy: 0.7631 - val_bbox_mean_squared_error: 0.1556\n",
      "Epoch 26/200\n",
      " - 83s - loss: 3.7676 - img_loss: 1.9519 - bbox_loss: 0.0201 - img_acc: 0.4554 - img_top_k_categorical_accuracy: 0.7827 - bbox_mean_squared_error: 0.0201 - val_loss: 4.1456 - val_img_loss: 2.1818 - val_bbox_loss: 0.1691 - val_img_acc: 0.4044 - val_img_top_k_categorical_accuracy: 0.7869 - val_bbox_mean_squared_error: 0.1691\n",
      "Epoch 27/200\n",
      " - 83s - loss: 3.7937 - img_loss: 1.9791 - bbox_loss: 0.0207 - img_acc: 0.4171 - img_top_k_categorical_accuracy: 0.7816 - bbox_mean_squared_error: 0.0207 - val_loss: 4.2692 - val_img_loss: 2.3084 - val_bbox_loss: 0.1677 - val_img_acc: 0.4019 - val_img_top_k_categorical_accuracy: 0.7806 - val_bbox_mean_squared_error: 0.1677\n",
      "Epoch 28/200\n",
      " - 83s - loss: 3.8288 - img_loss: 2.0163 - bbox_loss: 0.0203 - img_acc: 0.4100 - img_top_k_categorical_accuracy: 0.7783 - bbox_mean_squared_error: 0.0203 - val_loss: 4.4051 - val_img_loss: 2.4526 - val_bbox_loss: 0.1613 - val_img_acc: 0.3412 - val_img_top_k_categorical_accuracy: 0.7275 - val_bbox_mean_squared_error: 0.1613\n",
      "Epoch 29/200\n",
      " - 82s - loss: 3.7966 - img_loss: 1.9854 - bbox_loss: 0.0208 - img_acc: 0.4480 - img_top_k_categorical_accuracy: 0.7706 - bbox_mean_squared_error: 0.0208 - val_loss: 4.2685 - val_img_loss: 2.3233 - val_bbox_loss: 0.1557 - val_img_acc: 0.4306 - val_img_top_k_categorical_accuracy: 0.7631 - val_bbox_mean_squared_error: 0.1557\n",
      "Epoch 30/200\n",
      " - 83s - loss: 3.6963 - img_loss: 1.8881 - bbox_loss: 0.0195 - img_acc: 0.4785 - img_top_k_categorical_accuracy: 0.8161 - bbox_mean_squared_error: 0.0195 - val_loss: 4.3291 - val_img_loss: 2.3858 - val_bbox_loss: 0.1555 - val_img_acc: 0.4188 - val_img_top_k_categorical_accuracy: 0.7900 - val_bbox_mean_squared_error: 0.1555\n",
      "Epoch 31/200\n",
      " - 82s - loss: 3.8248 - img_loss: 2.0166 - bbox_loss: 0.0213 - img_acc: 0.4322 - img_top_k_categorical_accuracy: 0.7641 - bbox_mean_squared_error: 0.0213 - val_loss: 4.3922 - val_img_loss: 2.4393 - val_bbox_loss: 0.1668 - val_img_acc: 0.4006 - val_img_top_k_categorical_accuracy: 0.7606 - val_bbox_mean_squared_error: 0.1668\n",
      "Epoch 32/200\n",
      " - 83s - loss: 3.8138 - img_loss: 2.0087 - bbox_loss: 0.0199 - img_acc: 0.4296 - img_top_k_categorical_accuracy: 0.7689 - bbox_mean_squared_error: 0.0199 - val_loss: 4.2223 - val_img_loss: 2.2582 - val_bbox_loss: 0.1798 - val_img_acc: 0.4406 - val_img_top_k_categorical_accuracy: 0.7900 - val_bbox_mean_squared_error: 0.1798\n",
      "Epoch 33/200\n",
      " - 83s - loss: 3.6908 - img_loss: 1.8887 - bbox_loss: 0.0187 - img_acc: 0.4649 - img_top_k_categorical_accuracy: 0.7874 - bbox_mean_squared_error: 0.0187 - val_loss: 4.2434 - val_img_loss: 2.3006 - val_bbox_loss: 0.1602 - val_img_acc: 0.4256 - val_img_top_k_categorical_accuracy: 0.7606 - val_bbox_mean_squared_error: 0.1602\n",
      "Epoch 34/200\n",
      " - 83s - loss: 3.7175 - img_loss: 1.9162 - bbox_loss: 0.0195 - img_acc: 0.4676 - img_top_k_categorical_accuracy: 0.8031 - bbox_mean_squared_error: 0.0195 - val_loss: 4.1729 - val_img_loss: 2.2483 - val_bbox_loss: 0.1437 - val_img_acc: 0.4531 - val_img_top_k_categorical_accuracy: 0.7688 - val_bbox_mean_squared_error: 0.1437\n",
      "Epoch 35/200\n",
      " - 83s - loss: 3.8870 - img_loss: 2.0852 - bbox_loss: 0.0219 - img_acc: 0.4060 - img_top_k_categorical_accuracy: 0.7509 - bbox_mean_squared_error: 0.0219 - val_loss: 4.2643 - val_img_loss: 2.3321 - val_bbox_loss: 0.1530 - val_img_acc: 0.4138 - val_img_top_k_categorical_accuracy: 0.7906 - val_bbox_mean_squared_error: 0.1530\n",
      "Epoch 36/200\n",
      " - 83s - loss: 3.6757 - img_loss: 1.8779 - bbox_loss: 0.0195 - img_acc: 0.4657 - img_top_k_categorical_accuracy: 0.8215 - bbox_mean_squared_error: 0.0195 - val_loss: 4.3881 - val_img_loss: 2.4600 - val_bbox_loss: 0.1507 - val_img_acc: 0.3912 - val_img_top_k_categorical_accuracy: 0.7881 - val_bbox_mean_squared_error: 0.1507\n",
      "Epoch 37/200\n",
      " - 83s - loss: 3.6159 - img_loss: 1.8209 - bbox_loss: 0.0184 - img_acc: 0.5106 - img_top_k_categorical_accuracy: 0.8121 - bbox_mean_squared_error: 0.0184 - val_loss: 4.4051 - val_img_loss: 2.4482 - val_bbox_loss: 0.1812 - val_img_acc: 0.4200 - val_img_top_k_categorical_accuracy: 0.7738 - val_bbox_mean_squared_error: 0.1812\n",
      "Epoch 38/200\n",
      " - 83s - loss: 3.6368 - img_loss: 1.8427 - bbox_loss: 0.0192 - img_acc: 0.4701 - img_top_k_categorical_accuracy: 0.8150 - bbox_mean_squared_error: 0.0192 - val_loss: 4.2852 - val_img_loss: 2.3688 - val_bbox_loss: 0.1422 - val_img_acc: 0.3987 - val_img_top_k_categorical_accuracy: 0.7712 - val_bbox_mean_squared_error: 0.1422\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 39/200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_model.fit_generator(custom_generator(train_iterator),\n",
    "                          steps_per_epoch=250,\n",
    "                          epochs=200, validation_data=custom_generator(test_iterator),\n",
    "                          validation_steps=200,\n",
    "                          verbose=2,\n",
    "                          shuffle=True,\n",
    "                          use_multiprocessing=True,\n",
    "                          callbacks=[lr_reducer, checkpoint, early_stopper, tensorboard],\n",
    "                          workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 12627564580792739277), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 7861607336171332098), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:1, XLA_GPU, 17179869184, 13453473543620267060), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:2, XLA_GPU, 17179869184, 3391956349241482811), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:3, XLA_GPU, 17179869184, 11378612580382878187), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:4, XLA_GPU, 17179869184, 13222914120819690665), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:5, XLA_GPU, 17179869184, 8540793340616744063), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:6, XLA_GPU, 17179869184, 13863235290536309851), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:7, XLA_GPU, 17179869184, 4196259627609262878), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16615217446382736280), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 11323765556, 12786167581016013174), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:1, GPU, 11323765556, 7232660362384994032), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:2, GPU, 11323765556, 12577418999798667618), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:3, GPU, 11323703296, 1839790140192784857), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:4, GPU, 11323765556, 11967269025184174167), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:5, GPU, 11323765556, 10309205474909813042), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:6, GPU, 11323765556, 16889613425017997281), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:7, GPU, 11323765556, 16946601006073852227)]\n"
     ]
    }
   ],
   "source": [
    "print(sess.list_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
